# -*- coding: utf-8 -*-
"""ArcNemesis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jU5cQC10yLoGkTQ3ioLFWSlTDk9HIhOv
"""

!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score

from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    GenerationConfig
)
from tqdm import tqdm
from trl import SFTTrainer
import torch
import time
import pandas as pd
import numpy as np
from huggingface_hub import interpreter_login

interpreter_login()

import os
# disable Weights and Biases
os.environ['WANDB_DISABLED']="true"

huggingface_dataset_name = "suryanshp1/kali-linux-pentesting-data"
dataset = load_dataset(huggingface_dataset_name)

dataset['train'][0]

compute_dtype = getattr(torch, "float16")
bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type='nf4',
        bnb_4bit_compute_dtype=compute_dtype,
        bnb_4bit_use_double_quant=False,
    )

import torch
print(torch.cuda.is_available())
print(torch.cuda.device_count())

model_name='teknium/OpenHermes-2.5-Mistral-7B'
device_map = {"": 0}
original_model = AutoModelForCausalLM.from_pretrained(model_name,
                                                      device_map=device_map,
                                                      quantization_config=bnb_config,
                                                      trust_remote_code=True,
                                                      use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side="left",add_eos_token=True,add_bos_token=True,use_fast=False)
tokenizer.pad_token = tokenizer.eos_token

from datasets import load_dataset

dataset = load_dataset("suryanshp1/kali-linux-pentesting-data")
print(dataset)
print(dataset['train'][0])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from transformers import set_seed
# seed = 42
# set_seed(seed)
# 
# index = 10
# 
# prompt = dataset['train'][index]['instruction']
# summary = dataset['train'][index]['response']
# 
# formatted_prompt = f"Instruct: Summarize the following conversation.\n{prompt}\nOutput:\n"
# 
# # Define the gen function to generate text using the model and tokenizer
# def gen(model, prompt, max_length):
#     # Tokenize the prompt
#     inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True).to(model.device)
#     # Generate text
#     output_sequences = model.generate(
#         **inputs,
#         max_new_tokens=max_length,
#         num_return_sequences=1,
#         no_repeat_ngram_size=2,
#         do_sample=True,
#         temperature=0.7,
#         top_k=50,
#         top_p=0.95
#     )
#     # Decode the generated tokens
#     generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)
#     # Return the generated text as a list (to match the original code's expected output format)
#     return [generated_text]
# 
# res = gen(original_model,formatted_prompt,100,)
# #print(res[0])
# # Ensure there's an 'Output:\n' in the response before splitting to avoid IndexError
# if 'Output:\n' in res[0]:
#     output = res[0].split('Output:\n')[1]
# else:
#     # Handle cases where the model doesn't produce the expected output format
#     output = res[0]
# 
# 
# dash_line = '-'.join('' for x in range(100))
# print(dash_line)
# print(f'INPUT PROMPT:\n{formatted_prompt}')
# print(dash_line)
# # Updated the label to match the prompt's intent
# print(f'EXPECTED OUTPUT:\n{summary}\n')
# print(dash_line)
# print(f'MODEL GENERATION - ZERO SHOT:\n{output}')

def create_prompt_formats(sample):
    """
    Format various fields of the sample ('instruction','output')
    Then concatenate them using two newline characters
    :param sample: Sample dictionnary
    """
    INTRO_BLURB = "Below is an instruction that describes a task. Write a response that appropriately completes the request."
    INSTRUCTION_KEY = "### Instruct: Summarize the below conversation."
    RESPONSE_KEY = "### Output:"
    END_KEY = "### End"

    blurb = f"\n{INTRO_BLURB}"
    instruction = f"{INSTRUCTION_KEY}"
    input_context = f"{sample['dialogue']}" if sample["dialogue"] else None
    response = f"{RESPONSE_KEY}\n{sample['summary']}"
    end = f"{END_KEY}"

    parts = [part for part in [blurb, instruction, input_context, response, end] if part]

    formatted_prompt = "\n\n".join(parts)
    sample["text"] = formatted_prompt

    return sample

from functools import partial

# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py
def get_max_length(model):
    conf = model.config
    max_length = None
    for length_setting in ["n_positions", "max_position_embeddings", "seq_length"]:
        max_length = getattr(model.config, length_setting, None)
        if max_length:
            print(f"Found max lenth: {max_length}")
            break
    if not max_length:
        max_length = 1024
        print(f"Using default max length: {max_length}")
    return max_length


def preprocess_batch(batch, tokenizer, max_length):
    """
    Tokenizing a batch
    """
    return tokenizer(
        batch["text"],
        max_length=max_length,
        truncation=True,
    )

# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py
def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):
    """Format & tokenize it so it is ready for training
    :param tokenizer (AutoTokenizer): Model Tokenizer
    :param max_length (int): Maximum number of tokens to emit from tokenizer
    """

    # Add prompt to each sample
    print("Preprocessing dataset...")
    dataset = dataset.map(create_prompt_formats)#, batched=True)

    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields
    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)
    dataset = dataset.map(
        _preprocessing_function,
        batched=True,
        remove_columns=['id', 'topic', 'dialogue', 'summary'],
    )

    # Filter out samples that have input_ids exceeding max_length
    dataset = dataset.filter(lambda sample: len(sample["input_ids"]) < max_length)

    # Shuffle dataset
    dataset = dataset.shuffle(seed=seed)

    return dataset

def create_prompt_formats(sample):
    """
    Format various fields of the sample ('instruction','output')
    Then concatenate them using two newline characters
    :param sample: Sample dictionnary
    """
    INTRO_BLURB = "Below is an instruction that describes a task. Write a response that appropriately completes the request."
    INSTRUCTION_KEY = "### Instruct: Summarize the below conversation."
    RESPONSE_KEY = "### Output:"
    END_KEY = "### End"

    blurb = f"\n{INTRO_BLURB}"
    # Use 'instruction' key instead of 'dialogue' for the input context
    instruction = f"{INSTRUCTION_KEY}\n{sample['instruction']}"
    # Remove the input_context part as 'instruction' contains the prompt
    # input_context = f"{sample['dialogue']}" if sample["dialogue"] else None
    # Use 'response' key instead of 'summary' for the response
    response = f"{RESPONSE_KEY}\n{sample['response']}"
    end = f"{END_KEY}"

    # Combine the parts, excluding input_context as it's integrated into instruction
    parts = [part for part in [blurb, instruction, response, end] if part]


    formatted_prompt = "\n\n".join(parts)
    sample["text"] = formatted_prompt

    return sample

# 2 - Using the prepare_model_for_kbit_training method from PEFT
from peft import prepare_model_for_kbit_training

# Preparing the Model for QLoRA
original_model = prepare_model_for_kbit_training(original_model)

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

config = LoraConfig(
    r=32, #Rank
    lora_alpha=32,
    target_modules=[
        'q_proj',
        'k_proj',
        'v_proj',
        'dense'
    ],
    bias="none",
    lora_dropout=0.05,  # Conventional
    task_type="CAUSAL_LM",
)

# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning
original_model.gradient_checkpointing_enable()

peft_model = get_peft_model(original_model, config)

def print_number_of_trainable_model_parameters(model):
    trainable_model_params = 0
    all_model_params = 0
    for _, param in model.named_parameters():
        all_model_params += param.numel()
        if param.requires_grad:
            trainable_model_params += param.numel()
    return f"trainable model parameters: {trainable_model_params}\nall model parameters: {all_model_params}\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%"

print(print_number_of_trainable_model_parameters(peft_model))

# Commented out IPython magic to ensure Python compatibility.
# # %%
# !pip install -q -U bitsandbytes transformers==4.30.2 peft accelerate datasets scipy einops evaluate trl rouge_score
# # %%
# from IPython import get_ipython
# from IPython.display import display
# # %%
# from datasets import load_dataset
# from transformers import (
#     AutoModelForCausalLM,
#     AutoTokenizer,
#     BitsAndBytesConfig,
#     HfArgumentParser,
#     AutoTokenizer,
#     TrainingArguments,
#     Trainer,
#     GenerationConfig
# )
# from tqdm import tqdm
# from trl import SFTTrainer
# import torch
# import time
# import pandas as pd
# import numpy as np
# from huggingface_hub import interpreter_login
# 
# interpreter_login()
# # %%
# import os
# # disable Weights and Biases
# os.environ['WANDB_DISABLED']="true"
# # %%
# huggingface_dataset_name = "suryanshp1/kali-linux-pentesting-data"
# dataset = load_dataset(huggingface_dataset_name)
# # %%
# dataset['train'][0]
# # %%
# compute_dtype = getattr(torch, "float16")
# bnb_config = BitsAndBytesConfig(
#         load_in_4bit=True,
#         bnb_4bit_quant_type='nf4',
#         bnb_4bit_compute_dtype=compute_dtype,
#         bnb_4bit_use_double_quant=False,
#     )
# # %%
# model_name='teknium/OpenHermes-2.5-Mistral-7B'
# device_map = {"": 0}
# original_model = AutoModelForCausalLM.from_pretrained(model_name,
#                                                       device_map=device_map,
#                                                       quantization_config=bnb_config,
#                                                       trust_remote_code=True,
#                                                       use_auth_token=True)
# # %%
# tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side="left",add_eos_token=True,add_bos_token=True,use_fast=False)
# tokenizer.pad_token = tokenizer.eos_token
# # %%
# from datasets import load_dataset
# 
# dataset = load_dataset("suryanshp1/kali-linux-pentesting-data")
# print(dataset)
# print(dataset['train'][0])
# # %%
# %%time
# from transformers import set_seed
# seed = 42
# set_seed(seed)
# 
# index = 10
# 
# prompt = dataset['train'][index]['instruction']
# summary = dataset['train'][index]['response']
# 
# formatted_prompt = f"Instruct: Summarize the following conversation.\n{prompt}\nOutput:\n"
# 
# # Define the gen function to generate text using the model and tokenizer
# def gen(model, prompt, max_length):
#     # Tokenize the prompt
#     inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True).to(model.device)
#     # Generate text
#     output_sequences = model.generate(
#         **inputs,
#         max_new_tokens=max_length,
#         num_return_sequences=1,
#         no_repeat_ngram_size=2,
#         do_sample=True,
#         temperature=0.7,
#         top_k=50,
#         top_p=0.95
#     )
#     # Decode the generated tokens
#     generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)
#     # Return the generated text as a list (to match the original code's expected output format)
#     return [generated_text]
# 
# res = gen(original_model,formatted_prompt,100,)
# #print(res[0])
# # Ensure there's an 'Output:\n' in the response before splitting to avoid IndexError
# if 'Output:\n' in res[0]:
#     output = res[0].split('Output:\n')[1]
# else:
#     # Handle cases where the model doesn't produce the expected output format
#     output = res[0]
# 
# 
# dash_line = '-'.join('' for x in range(100))
# print(dash_line)
# print(f'INPUT PROMPT:\n{formatted_prompt}')
# print(dash_line)
# # Updated the label to match the prompt's intent
# print(f'EXPECTED OUTPUT:\n{summary}\n')
# print(dash_line)
# print(f'MODEL GENERATION - ZERO SHOT:\n{output}')
# # %%
# def create_prompt_formats(sample):
#     """
#     Format various fields of the sample ('instruction','output')
#     Then concatenate them using two newline characters
#     :param sample: Sample dictionnary
#     """
#     INTRO_BLURB = "Below is an instruction that describes a task. Write a response that appropriately completes the request."
#     INSTRUCTION_KEY = "### Instruct: Summarize the below conversation."
#     RESPONSE_KEY = "### Output:"
#     END_KEY = "### End"
# 
#     blurb = f"\n{INTRO_BLURB}"
#     instruction = f"{INSTRUCTION_KEY}"
#     input_context = f"{sample['dialogue']}" if sample["dialogue"] else None
#     response = f"{RESPONSE_KEY}\n{sample['summary']}"
#     end = f"{END_KEY}"
# 
#     parts = [part for part in [blurb, instruction, input_context, response, end] if part]
# 
#     formatted_prompt = "\n\n".join(parts)
#     sample["text"] = formatted_prompt
# 
#     return sample
# # %%
# from functools import partial
# 
# # SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py
# def get_max_length(model):
#     conf = model.config
#     max_length = None
#     for length_setting in ["n_positions", "max_position_embeddings", "seq_length"]:
#         max_length = getattr(model.config, length_setting, None)
#         if max_length:
#             print(f"Found max lenth: {max_length}")
#             break
#     if not max_length:
#         max_length = 1024
#         print(f"Using default max length: {max_length}")
#     return max_length
# 
# 
# def preprocess_batch(batch, tokenizer, max_length):
#     """
#     Tokenizing a batch
#     """
#     return tokenizer(
#         batch["text"],
#         max_length=max_length,
#         truncation=True,
#     )
# 
# # SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py
# def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):
#     """Format & tokenize it so it is ready for training
#     :param tokenizer (AutoTokenizer): Model Tokenizer
#     :param max_length (int): Maximum number of tokens to emit from tokenizer
#     """
# 
#     # Add prompt to each sample
#     print("Preprocessing dataset...")
#     dataset = dataset.map(create_prompt_formats)#, batched=True)
# 
#     # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields
#     _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)
#     dataset = dataset.map(
#         _preprocessing_function,
#         batched=True,
#         remove_columns=['id', 'topic', 'dialogue', 'summary'],
#     )
# 
#     # Filter out samples that have input_ids exceeding max_length
#     dataset = dataset.filter(lambda sample: len(sample["input_ids"]) < max_length)
# 
#     # Shuffle dataset
#     dataset = dataset.shuffle(seed=seed)
# 
#     return dataset
# # %%
# def create_prompt_formats(sample):
#     """
#     Format various fields of the sample ('instruction','output')
#     Then concatenate them using two newline characters
#     :param sample: Sample dictionnary
#     """
#     INTRO_BLURB = "Below is an instruction that describes a task. Write a response that appropriately completes the request."
#     INSTRUCTION_KEY = "### Instruct: Summarize the below conversation."
#     RESPONSE_KEY = "### Output:"
#     END_KEY = "### End"
# 
#     blurb = f"\n{INTRO_BLURB}"
#     # Use 'instruction' key instead of 'dialogue' for the input context
#     instruction = f"{INSTRUCTION_KEY}\n{sample['instruction']}"
#     # Remove the input_context part as 'instruction' contains the prompt
#     # input_context = f"{sample['dialogue']}" if sample["dialogue"] else None
#     # Use 'response' key instead of 'summary' for the response
#     response = f"{RESPONSE_KEY}\n{sample['response']}"
#     end = f"{END_KEY}"
# 
#     # Combine the parts, excluding input_context as it's integrated into instruction
#     parts = [part for part in [blurb, instruction, response, end] if part]
# 
# 
#     formatted_prompt = "\n\n".join(parts)
#     sample["text"] = formatted_prompt
# 
#     return sample
# # %%
# # 2 - Using the prepare_model_for_kbit_training method from PEFT
# from peft import prepare_model_for_kbit_training
# 
# # Preparing the Model for QLoRA
# original_model = prepare_model_for_kbit_training(original_model)
# # %%
# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
# 
# config = LoraConfig(
#     r=32, #Rank
#     lora_alpha=32,
#     target_modules=[
#         'q_proj',
#         'k_proj',
#         'v_proj',
#         'dense'
#     ],
#     bias="none",
#     lora_dropout=0.05,  # Conventional
#     task_type="CAUSAL_LM",
# )
# 
# # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning
# original_model.gradient_checkpointing_enable()
# 
# peft_model = get_peft_model(original_model, config)
# # %%
# def print_number_of_trainable_model_parameters(model):
#     trainable_model_params = 0
#     all_model_params = 0
#     for _, param in model.named_parameters():
#         all_model_params += param.numel()
#         if param.requires_grad:
#             trainable_model_params += param.numel()
#     return f"trainable model parameters: {trainable_model_params}\nall model parameters: {all_model_params}\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%"
# 
# print(print_number_of_trainable_model_parameters(peft_model))
# # %%
# output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'
# import transformers
# 
# peft_training_args = TrainingArguments(
#     output_dir = output_dir,
#     warmup_steps=1,
#     per_device_train_batch_size=1,
#     gradient_accumulation_steps=4,
#     max_steps=1000,
#     learning_rate=2e-4,
#     optim="paged_adamw_8bit",
#     logging_steps=25,
#     logging_dir="./logs",
#     save_strategy="steps",
#     save_steps=25,
#     evaluation_strategy="steps", # Ensure this is explicitly set
#     eval_steps=25,
#     do_eval=True,
#     gradient_checkpointing=True,
#     report_to="none",
#     overwrite_output_dir = 'True',
#     group_by_length=True,
# )
# 
# peft_model.config.use_cache = False
# 
# peft_trainer = transformers.Trainer(
#     model=peft_model,
#     train_dataset=train_dataset, # train_dataset is not defined yet. This will cause a NameError later.
#     eval_dataset=eval_dataset,   # eval_dataset is not defined yet. This will cause a NameError later.
#     args=peft_training_args,
#     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
# )

from functools import partial
from datasets import load_dataset, DatasetDict # Import DatasetDict for type hinting

# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py
def get_max_length(model):
    conf = model.config
    max_length = None
    for length_setting in ["n_positions", "max_position_embeddings", "seq_length"]:
        max_length = getattr(model.config, length_setting, None)
        if max_length:
            print(f"Found max lenth: {max_length}")
            break
    if not max_length:
        max_length = 1024
        print(f"Using default max length: {max_length}")
    return max_length


def preprocess_batch(batch, tokenizer, max_length):
    """
    Tokenizing a batch
    """
    return tokenizer(
        batch["text"],
        max_length=max_length,
        truncation=True,
    )

# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py
def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: DatasetDict):
    """Format & tokenize it so it is ready for training
    :param tokenizer (AutoTokenizer): Model Tokenizer
    :param max_length (int): Maximum number of tokens to emit from tokenizer
    :param seed (int): Random seed for shuffling
    :param dataset (DatasetDict): The input dataset
    """

    # Add prompt to each sample using the corrected create_prompt_formats
    print("Preprocessing dataset...")
    # Assuming the create_prompt_formats function defined in ipython-input-17 is the correct one to use.
    # We need to make sure this function is available in the current scope.
    # For clarity and to avoid confusion with the previous definition, let's redefine or ensure the correct one is used.
    # Since the notebook runs sequentially, the last defined create_prompt_formats (ipython-input-17) will be used here.
    # The function defined in ipython-input-17 uses 'instruction' and 'response'.

    dataset = dataset.map(create_prompt_formats)

    # Apply preprocessing to each batch of the dataset & and remove the original columns
    # The columns to remove are the original ones used to create the 'text' column,
    # plus any other columns that are not needed for training.
    # Based on the dataset structure and the create_prompt_formats function in ipython-input-17,
    # the original columns are 'id', 'topic', 'instruction', and 'response'.
    columns_to_remove = ['id', 'topic', 'instruction', 'response']

    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)
    dataset = dataset.map(
        _preprocessing_function,
        batched=True,
        remove_columns=columns_to_remove,
    )

    # Filter out samples that have input_ids exceeding max_length
    dataset = dataset.filter(lambda sample: len(sample["input_ids"]) < max_length)

    # Shuffle dataset
    dataset = dataset.shuffle(seed=seed)

    return dataset

# Now, include the code from ipython-input-26 and call the corrected preprocess_dataset
# Ensure create_prompt_formats from ipython-input-17 is also included in the same cell or defined before calling preprocess_dataset.

# Include the corrected create_prompt_formats function here for clarity
def create_prompt_formats(sample):
    """
    Format various fields of the sample ('instruction','output')
    Then concatenate them using two newline characters
    :param sample: Sample dictionnary
    """
    INTRO_BLURB = "Below is an instruction that describes a task. Write a response that appropriately completes the request."
    INSTRUCTION_KEY = "### Instruct: Summarize the below conversation."
    RESPONSE_KEY = "### Output:"
    END_KEY = "### End"

    blurb = f"\n{INTRO_BLURB}"
    # Use 'instruction' key for the input context
    instruction = f"{INSTRUCTION_KEY}\n{sample['instruction']}"
    # Use 'response' key for the response
    response = f"{RESPONSE_KEY}\n{sample['response']}"
    end = f"{END_KEY}"

    # Combine the parts
    parts = [part for part in [blurb, instruction, response, end] if part]


    formatted_prompt = "\n\n".join(parts)
    sample["text"] = formatted_prompt

    return sample

output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'
import transformers
from sklearn.model_selection import train_test_split
import datasets # Import datasets module
from functools import partial # Import partial
from transformers import AutoTokenizer, TrainingArguments, Trainer, GenerationConfig # Ensure necessary imports
import time # Import time for output_dir
from datasets import load_dataset # Import load_dataset

# Ensure the corrected create_prompt_formats and preprocess_dataset are defined or available here

def create_prompt_formats(sample):
    """
    Format various fields of the sample ('instruction','output')
    Then concatenate them using two newline characters
    :param sample: Sample dictionnary
    """
    INTRO_BLURB = "Below is an instruction that describes a task. Write a response that appropriately completes the request."
    INSTRUCTION_KEY = "### Instruct: Summarize the below conversation."
    RESPONSE_KEY = "### Output:"
    END_KEY = "### End"

    blurb = f"\n{INTRO_BLURB}"
    # Use 'instruction' key for the input context
    instruction = f"{INSTRUCTION_KEY}\n{sample['instruction']}"
    # Use 'response' key for the response
    response = f"{RESPONSE_KEY}\n{sample['response']}"
    end = f"{END_KEY}"

    # Combine the parts
    parts = [part for part in [blurb, instruction, response, end] if part]


    formatted_prompt = "\n\n".join(parts)
    # The function should return a dictionary containing the new 'text' column
    # along with any original columns you want to keep.
    # Since we want to remove the original columns later, just returning 'text' is fine here.
    sample["text"] = formatted_prompt
    return sample


def preprocess_batch(batch, tokenizer, max_length):
    """
    Tokenizing a batch
    """
    # The batch here will contain the 'text' column added by create_prompt_formats
    return tokenizer(
        batch["text"],
        max_length=max_length,
        truncation=True,
    )


def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: datasets.DatasetDict):
    """Format & tokenize it so it is ready for training
    :param tokenizer (AutoTokenizer): Model Tokenizer
    :param max_length (int): Maximum number of tokens to emit from tokenizer
    :param seed (int): Random seed for shuffling
    :param dataset (DatasetDict): The input dataset
    """

    # Add prompt to each sample using the corrected create_prompt_formats
    print("Preprocessing dataset...")
    # Apply create_prompt_formats first to add the 'text' column.
    # By default, map will keep original columns unless specified otherwise.
    dataset_with_text = dataset.map(create_prompt_formats)

    # Define columns to remove - these are the original columns plus the intermediate 'text' column
    # We will remove the original columns AND the 'text' column after tokenization
    # Check which columns exist in the dataset_with_text before attempting to remove
    all_columns_to_consider = ['id', 'topic', 'instruction', 'response', 'text']
    columns_to_remove = [col for col in all_columns_to_consider if col in dataset_with_text['train'].column_names]


    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)

    # Apply tokenization and remove the specified columns
    processed_dataset = dataset_with_text.map( # Call map on the dataset with the 'text' column
        _preprocessing_function,
        batched=True,
        remove_columns=columns_to_remove,
    )


    # Filter out samples that have input_ids exceeding max_length
    processed_dataset = processed_dataset.filter(lambda sample: len(sample["input_ids"]) < max_length)

    # Shuffle dataset
    processed_dataset = processed_dataset.shuffle(seed=seed)

    return processed_dataset

def get_max_length(model):
    conf = model.config
    max_length = None
    for length_setting in ["n_positions", "max_position_embeddings", "seq_length"]:
        max_length = getattr(model.config, length_setting, None)
        if max_length:
            print(f"Found max lenth: {max_length}")
            break
    if not max_length:
        max_length = 1024
        print(f"Using default max length: {max_length}")
    return max_length

# Make sure 'output_dir' and 'seed' variables are defined before this cell is executed.
# They were defined in ipython-input-24 and ipython-input-13 respectively.

# Ensure output_dir and seed are defined if this cell is run independently
# You should run the previous cells to define these. Assuming they are defined.
# For safety, you could redefine them here if running this cell in isolation:
# output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'
# seed = 42 # Or the desired seed

# --- Added for robustness: Reload the dataset ---
huggingface_dataset_name = "suryanshp1/kali-linux-pentesting-data"
dataset = load_dataset(huggingface_dataset_name)
# -----------------------------------------------


# Get max length of the model
max_length = get_max_length(original_model)

# Preprocess the dataset - This call will now correctly use the dataset loaded earlier
processed_dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)

# Split the dataset into train and evaluation sets
# Access the 'train' split of the processed dataset to perform the train_test_split
train_eval_split = processed_dataset['train'].train_test_split(test_size=0.1, seed=seed)
train_dataset = train_eval_split['train']
eval_dataset = train_eval_split['test']

peft_training_args = TrainingArguments(
    output_dir = output_dir,
    warmup_steps=1,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    max_steps=1000,
    learning_rate=2e-4,
    optim="paged_adamw_8bit",
    logging_steps=25,
    logging_dir="./logs",
    save_strategy="steps",
    save_steps=25,
    # Changed evaluation_strategy to eval_strategy
    eval_strategy="steps",
    eval_steps=25,
    do_eval=True,
    gradient_checkpointing=True,
    report_to="none",
    overwrite_output_dir = 'True',
    group_by_length=True,
)

peft_model.config.use_cache = False

peft_trainer = transformers.Trainer(
    model=peft_model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    args=peft_training_args,
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

peft_trainer.train()

"""Model is Trained **now**"""

!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score

# 1. Save the trained PEFT model
# peft_trainer.train() has completed. Now save the final model.
# The path should be where you want to save the model.
# Use the output_dir defined earlier, or specify a new path.
output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}' # Redefining if needed, or use the existing output_dir
peft_model_path = f"{output_dir}/final_checkpoint" # Or a specific path
peft_model.save_pretrained(peft_model_path)

print(f"PEFT model saved to {peft_model_path}")

# 2. Load the saved PEFT model for inference
# Load the base model first
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Make sure the original model name and quantization config are available
model_name = 'teknium/OpenHermes-2.5-Mistral-7B'
compute_dtype = getattr(torch, "float16")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=False,
)
device_map = {"": 0}

base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map=device_map,
    trust_remote_code=True,
    use_auth_token=True # Use if authentication is required
)

# Load the tokenizer (make sure it's the same one used for training)
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, padding_side="left", add_eos_token=True, add_bos_token=True, use_fast=False)
tokenizer.pad_token = tokenizer.eos_token

# Load the PEFT model
# Use the path where you saved the PEFT model
peft_model_id = peft_model_path # Or the path to your saved adapter weights

# [1] Relevant information from the search result:
# "if it is for LoRA method using INT8, call the prepare_int8_model_for_training on the base model, then do the PeftModel. from_pretrained(base_model, peft_model_id)"
# Although the source mentions INT8 and prepare_int8_model_for_training, the general principle of loading the base model
# and then loading the PEFT weights using PeftModel.from_pretrained(base_model, peft_model_id) is applicable here for QLoRA (4-bit).
# You already used prepare_model_for_kbit_training which is appropriate for 4-bit.

model_for_inference = PeftModel.from_pretrained(base_model, peft_model_id)

# Set the model to evaluation mode
model_for_inference.eval()

print("PEFT model loaded successfully for inference.")

# 3. Merge PEFT weights with the base model (Optional step for easier deployment)
# This creates a new model with merged weights
# You can save this merged model to a new directory
# Note: Merging might increase memory usage.
# merged_model = model_for_inference.merge_and_unload()
# merged_model_path = f"./merged_model_{str(int(time.time()))}"
# merged_model.save_pretrained(merged_model_path)
# tokenizer.save_pretrained(merged_model_path) # Also save the tokenizer with the merged model
# print(f"Merged model saved to {merged_model_path}")

# 4. Perform inference with the fine-tuned model
# Use the 'gen' function you defined earlier, but with the loaded fine-tuned model

# Example inference
index = 15 # Choose a different index from the dataset for testing
prompt = dataset['train'][index]['instruction']
summary = dataset['train'][index]['response']

# Reformat the prompt for the fine-tuned model based on your training format
# The training format was defined in the create_prompt_formats function you used for training.
# It looked like this:
# INTRO_BLURB
#
# INSTRUCTION_KEY\n{sample['instruction']}
#
# RESPONSE_KEY\n{sample['response']}
#
# END_KEY

INTRO_BLURB = "Below is an instruction that describes a task. Write a response that appropriately completes the request."
INSTRUCTION_KEY = "### Instruct: Summarize the below conversation."
RESPONSE_KEY = "### Output:"
END_KEY = "### End"

formatted_prompt_for_inference = f"{INTRO_BLURB}\n\n{INSTRUCTION_KEY}\n{prompt}\n\n{RESPONSE_KEY}\n" # We don't add the response and END_KEY here as we want the model to generate it.


print("\n" + '-'.join('' for x in range(100)))
print(f'INPUT PROMPT FOR INFERENCE:\n{formatted_prompt_for_inference}')
print('-'.join('' for x in range(100)))
print(f'EXPECTED OUTPUT:\n{summary}\n')
print('-'.join('' for x in range(100)))

# Generate response using the fine-tuned model
# Use the 'gen' function, but pass the loaded peft_model (model_for_inference)
res_tuned = gen(model_for_inference, formatted_prompt_for_inference, max_length=200) # Adjust max_length as needed

# Extract the generated output part, similar to how you did it before
if 'Output:\n' in res_tuned[0]:
    output_tuned = res_tuned[0].split('Output:\n', 1)[1] # Use split(, 1) to split only on the first occurrence
else:
    output_tuned = res_tuned[0] # Handle cases where the expected format is not present

print(f'MODEL GENERATION - FINE-TUNED:\n{output_tuned}')
print('-'.join('' for x in range(100)))

"""**Here Results Are Satisfactory**"""

merged_model = model_for_inference.merge_and_unload()

"""Once the model is trained successfully, we can use it for inference. Let’s now prepare the inference model by adding an adapter to the original Phi-2 model. Here, we are setting is_trainable=False because the plan is only to perform inference with this PEFT model."""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

base_model_id = "teknium/OpenHermes-2.5-Mistral-7B"
base_model = AutoModelForCausalLM.from_pretrained(base_model_id,
                                                      device_map='auto',
                                                      quantization_config=bnb_config,
                                                      trust_remote_code=True,
                                                      use_auth_token=True)

# 3. Merge PEFT weights with the base model (Optional step for easier deployment)
# This creates a new model with merged weights
# You can save this merged model to a new directory
# Note: Merging might increase memory usage.
merged_model = model_for_inference.merge_and_unload() # This is where the merging happens in memory
# merged_model_path = f"./merged_model_{str(int(time.time()))}" # This line defines a path
# merged_model.save_pretrained(merged_model_path) # This line saves the model to the path
# tokenizer.save_pretrained(merged_model_path) # Also save the tokenizer with the merged model
# print(f"Merged model saved to {merged_model_path}") # This line prints the save location

# Assuming merged_model is already created in memory from the previous step
merged_model_path = f"./merged_model_{str(int(time.time()))}" # Define a path to save the merged model
merged_model.save_pretrained(merged_model_path) # Save the merged model
tokenizer.save_pretrained(merged_model_path) # Save the tokenizer alongside the model

print(f"Merged model saved to {merged_model_path}")

"""Pusing the model to the hugging face"""

# %%
# Run this cell again to ensure you are logged in with a token with write permissions
from huggingface_hub import interpreter_login
interpreter_login()

# %%
# Assuming you have already run the code to save the merged_model to merged_model_path
# merged_model_path = f"./merged_model_{str(int(time.time()))}" # Path where you saved the merged model

# 1. Define the model name for your Hugging Face repository
# Replace 'your_model_name' with the desired name for your repository
# Make sure you are logged in to Hugging Face Hub using interpreter_login()
your_hf_username = "esCyanide"  # Replace with your actual Hugging Face username
your_model_repo_name = "ArcNemesis" # Replace with your desired model name

hf_model_repo_id = f"{your_hf_username}/{your_model_repo_name}"

# 2. Push the merged model to the Hugging Face Hub
print(f"\nPushing merged model to Hugging Face Hub: {hf_model_repo_id}")

# The save_pretrained method saves the model files.
# To push to the hub, you use the push_to_hub method of the saved model.
# You need to load the model again from the saved path or use the merged_model directly
# if it's still in memory and you saved it using save_pretrained to the specified path.

# Let's ensure we have the merged_model and tokenizer objects in memory
# Assuming the previous cell where you saved the merged_model to merged_model_path just ran.
# If you are running this cell independently, you would load them first:
from transformers import AutoModelForCausalLM, AutoTokenizer

print(f"Loading merged model from {merged_model_path} to push...")
# Use the Auto classes to load the merged model and tokenizer from the saved path
# Make sure the correct base model is loaded first if needed for structure, although
# AutoModelForCausalLM.from_pretrained(path) should load the saved configuration.
# If there were issues, explicitly load the base model config first.
# For now, let's assume loading directly from the merged_model_path works.
try:
    model_to_push = AutoModelForCausalLM.from_pretrained(merged_model_path)
except Exception as e:
    print(f"Error loading merged model directly from path: {e}")
    print("Attempting to load with trust_remote_code=True...")
    # Sometimes trust_remote_code is needed even when loading from a local path if the original model used it
    model_to_push = AutoModelForCausalLM.from_pretrained(merged_model_path, trust_remote_code=True)


# Similarly for tokenizer
try:
    tokenizer_to_push = AutoTokenizer.from_pretrained(merged_model_path)
except Exception as e:
     print(f"Error loading tokenizer directly from path: {e}")
     print("Attempting to load with trust_remote_code=True and use_fast=False...")
     tokenizer_to_push = AutoTokenizer.from_pretrained(merged_model_path, trust_remote_code=True, use_fast=False)


print("Pushing model and tokenizer to Hugging Face Hub...")
# Use push_to_hub on the loaded model and tokenizer
# Ensure you are pushing to the correct repo ID under your verified username
# The `token` argument to `push_to_hub` can also be used here, but `interpreter_login` is preferred.
model_to_push.push_to_hub(hf_model_repo_id)
tokenizer_to_push.push_to_hub(hf_model_repo_id)

print(f"Model successfully pushed to https://huggingface.co/{hf_model_repo_id}")

"""**Training With Keggel New training Dataset**"""

# %%
# Install the Kaggle API if you haven't already
!pip install -q kaggle

# %%
# Ensure you have your Kaggle API token configured
# You can upload your kaggle.json file (containing your username and key)
# to your notebook environment, typically under ~/.kaggle/
# If running in Colab, there are specific instructions for handling secrets.
# You might need to run this interactively the first time:
# from google.colab import files
# files.upload() # Upload kaggle.json

# After uploading kaggle.json, set permissions
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# %%
# Download the specific Kaggle dataset
# Replace 'kaggle-dataset-owner/kaggle-dataset-name' and 'your_csv_file.csv'
# with the actual owner/name of the dataset and the name of the CSV file within it.
# Example: !kaggle datasets download -d rtatman/conversationaldata -f conversational_data.csv
# If the dataset is large, you might only need to download a specific file with -f
kaggle_dataset_path = "kaggle-dataset-owner/kaggle-dataset-name" # REPLACE WITH ACTUAL DATASET PATH
csv_file_name = "your_csv_file.csv" # REPLACE WITH ACTUAL CSV FILE NAME

# Download the specified file(s)
# If you only need one file:
!kaggle datasets download -d {kaggle_dataset_path} -f {csv_file_name}
# If the dataset is just a single file or you need all of them:
# !kaggle datasets download -d {kaggle_dataset_path} --unzip # Use --unzip if it's a zip file

# After running this, the CSV file (e.g., your_csv_file.csv) should be in your current directory.

# %%
from datasets import load_dataset, DatasetDict, Dataset
import pandas as pd # You might find it helpful to inspect the CSV with pandas first

# Define the path to your downloaded CSV file
local_csv_path = csv_file_name # This should match the name of the downloaded file

# Load the CSV file into a Hugging Face Dataset
# The 'csv' loading script automatically detects CSV files
# The dataset will be loaded as a DatasetDict with a 'train' split by default
# If your CSV doesn't have a header, you might need to specify column names.
# If the data needs splitting, you can do it after loading.
try:
    dataset = load_dataset('csv', data_files=local_csv_path)
    print("Dataset loaded successfully from CSV.")
    print(dataset)
    # Inspect the first sample and column names to understand its structure
    print("First sample:", dataset['train'][0])
    print("Column names:", dataset['train'].column_names)

except Exception as e:
    print(f"Error loading CSV with load_dataset: {e}")
    # Fallback: Load with pandas first to debug structure issues
    try:
        df = pd.read_csv(local_csv_path)
        print("Successfully loaded CSV with pandas. Head:")
        print(df.head())
        print("Column names:", df.columns.tolist())
        # Convert pandas DataFrame to Hugging Face Dataset
        dataset = Dataset.from_pandas(df).to_dataset_dict(split='train')
        print("\nConverted pandas DataFrame to DatasetDict.")
        print(dataset)
        print("First sample:", dataset['train'][0])
        print("Column names:", dataset['train'].column_names)

    except Exception as pd_e:
        print(f"Error loading CSV with pandas either: {pd_e}")
        raise pd_e # Re-raise the error if both methods fail

# %%
# *** IMPORTANT ***
# Adapt the create_prompt_formats function based on your CSV column names.
# Replace 'instruction' and 'response' with the actual column names from your CSV
# that contain the input text (like questions or dialogue) and the target output text (like answers or summaries).

def create_prompt_formats(sample):
    """
    Format various fields of the sample based on your CSV column names.
    Then concatenate them using two newline characters.
    :param sample: Sample dictionary from the dataset.
    """
    INTRO_BLURB = "Below is an instruction that describes a task. Write a response that appropriately completes the request."
    INSTRUCTION_KEY = "### Instruct: Summarize the below conversation." # Or adapt this based on your task
    RESPONSE_KEY = "### Output:"
    END_KEY = "### End"

    blurb = f"\n{INTRO_BLURB}"

    # --- ADAPT THESE LINES ---
    # Assuming your CSV has columns like 'input_text_col' and 'output_text_col'
    # Replace 'your_input_column_name' and 'your_output_column_name'
    input_text_col_name = 'your_input_column_name' # REPLACE THIS
    output_text_col_name = 'your_output_column_name' # REPLACE THIS

    instruction = f"{INSTRUCTION_KEY}\n{sample[input_text_col_name]}"
    response = f"{RESPONSE_KEY}\n{sample[output_text_col_name]}"
    # -------------------------

    end = f"{END_KEY}"

    # Combine the parts
    # Adjust which parts are included based on how you want to format the prompt
    # For instruction-following like tasks, blurb, instruction, response, and end might be appropriate.
    # If it's a simpler task like text completion, you might only need instruction and response.
    parts = [part for part in [blurb, instruction, response, end] if part]


    formatted_prompt = "\n\n".join(parts)
    sample["text"] = formatted_prompt # Add the formatted text as a new column

    return sample

# %%
# *** IMPORTANT ***
# Adapt the preprocess_dataset function to remove the correct original columns
# based on your CSV file.

from functools import partial
# Ensure get_max_length and preprocess_batch are defined as in your original code
# (These functions should work without modification as they operate on the 'text' column)

def get_max_length(model):
    conf = model.config
    max_length = None
    for length_setting in ["n_positions", "max_position_embeddings", "seq_length"]:
        max_length = getattr(model.config, length_setting, None)
        if max_length:
            print(f"Found max lenth: {max_length}")
            break
    if not max_length:
        max_length = 1024
        print(f"Using default max length: {max_length}")
    return max_length


def preprocess_batch(batch, tokenizer, max_length):
    """
    Tokenizing a batch
    """
    return tokenizer(
        batch["text"],
        max_length=max_length,
        truncation=True,
    )

# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py
def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: DatasetDict):
    """Format & tokenize it so it is ready for training
    :param tokenizer (AutoTokenizer): Model Tokenizer
    :param max_length (int): Maximum number of tokens to emit from tokenizer
    :param seed (int): Random seed for shuffling
    :param dataset (DatasetDict): The input dataset (loaded from CSV)
    """

    # Add prompt to each sample using the adapted create_prompt_formats
    print("Preprocessing dataset...")
    # Apply create_prompt_formats first to add the 'text' column.
    dataset_with_text = dataset.map(create_prompt_formats)

    # --- ADAPT THESE LINES ---
    # Define columns to remove - these are the original columns from your CSV
    # plus the intermediate 'text' column *before* tokenization.
    # Check dataset_with_text['train'].column_names after the first map to see
    # the columns present (original CSV columns + 'text').
    # List all the original columns from your CSV file here.
    original_csv_columns = dataset['train'].column_names # Get the column names from the initial load

    # We want to remove all original columns *and* the intermediate 'text' column after the text is tokenized
    # but before passing to the trainer. The tokenizer will produce 'input_ids', 'attention_mask', etc.
    columns_to_remove_after_tokenization = original_csv_columns + ['text'] # Remove original columns + the 'text' column

    # Remove 'text' *only* if it was added successfully in the previous map step
    if 'text' not in dataset_with_text['train'].column_names:
         print("Warning: 'text' column not found after create_prompt_formats. Check your function.")
         # If 'text' wasn't added, just remove original columns
         columns_to_remove_after_tokenization = original_csv_columns

    print(f"Columns in dataset before tokenization map: {dataset_with_text['train'].column_names}")
    print(f"Columns to attempt removing after tokenization map: {columns_to_remove_after_tokenization}")

    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)

    # Apply tokenization and remove the specified columns
    processed_dataset = dataset_with_text.map(
        _preprocessing_function,
        batched=True,
        remove_columns=[col for col in columns_to_remove_after_tokenization if col in dataset_with_text['train'].column_names], # Only attempt to remove columns that actually exist
    )
    # -------------------------

    # Filter out samples that have input_ids exceeding max_length
    processed_dataset = processed_dataset.filter(lambda sample: len(sample["input_ids"]) < max_length)

    # Shuffle dataset
    processed_dataset = processed_dataset.shuffle(seed=seed)

    return processed_dataset

# %%
# Now you can call the preprocess_dataset function with your loaded CSV dataset
# Ensure your tokenizer, model, and seed are defined from previous cells

# Ensure output_dir and seed are defined
# output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}' # Example
# seed = 42 # Example

# Get max length of the model
max_length = get_max_length(original_model) # Use original_model or peft_model if you're resuming training

# Preprocess the dataset - This call will now use the dataset loaded from CSV
processed_dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)

# Split the dataset into train and evaluation sets
# Access the 'train' split of the processed dataset (which comes from the CSV load)
# If you need a validation set, it's good practice to split.
train_eval_split = processed_dataset['train'].train_test_split(test_size=0.1, seed=seed) # Adjust test_size as needed
train_dataset = train_eval_split['train']
eval_dataset = train_eval_split['test']

print("\nProcessed train dataset:", train_dataset)
print("Processed eval dataset:", eval_dataset)

# %%
# Now you can use train_dataset and eval_dataset in your Trainer or SFTTrainer
# Your TrainingArguments and Trainer setup should be similar to before,
# but ensure you are passing the newly created train_dataset and eval_dataset.

# Example Trainer setup (assuming peft_model and peft_training_args are defined)
# peft_trainer = transformers.Trainer(
#     model=peft_model,
#     train_dataset=train_dataset,
#     eval_dataset=eval_dataset,
#     args=peft_training_args,
#     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
# )

# And then call peft_trainer.train()

# %%
# This cell loads the CSV - you've confirmed it works.
# You only need to run this once unless the file or path changes.
from datasets import load_dataset, DatasetDict, Dataset
import pandas as pd

local_csv_path = "/content/keggle/combined_pen_testing_tools.csv"

try:
    dataset = load_dataset('csv', data_files=local_csv_path, split='train')
    dataset = DatasetDict({'train': dataset}) # Wrap in DatasetDict
    print("Dataset loaded successfully from CSV.")
    print(dataset)
    print("\nFirst sample:")
    print(dataset['train'][0])
    print("\nColumn names:")
    print(dataset['train'].column_names)

except Exception as e:
    print(f"Error loading CSV with load_dataset: {e}")
    try:
        df = pd.read_csv(local_csv_path)
        print("\nSuccessfully loaded CSV with pandas. Head:")
        print(df.head())
        print("\nColumn names (from pandas):")
        print(df.columns.tolist())
        dataset = Dataset.from_pandas(df).to_dataset_dict(split='train')
        print("\nConverted pandas DataFrame to DatasetDict.")
        print(dataset)
        print("\nFirst sample (after conversion):")
        print(dataset['train'][0])
        print("\nColumn names (after conversion):")
        print(dataset['train'].column_names)
    except Exception as pd_e:
        print(f"Error loading CSV with pandas either: {pd_e}")
        raise pd_e

# %%
# *** CORRECTED COLUMN NAME HERE ***
# Adapt the create_prompt_formats function based on your CSV column names
# ('Tool', 'Category', 'Command', 'Description', 'Example_Usage')
# and the task you want the model to perform.
# Input: Tool, Category, Description
# Output: Command and Example Usage.

def create_prompt_formats(sample):
    """
    Format the sample based on CSV columns ('Tool', 'Category', 'Description',
    'Command', 'Example_Usage') into a prompt-response pair.
    :param sample: Sample dictionary from the dataset.
    """
    INTRO_BLURB = "Below is information about a penetration testing tool. Provide the command and an example usage."
    INSTRUCTION_KEY = "### Tool Info:"
    RESPONSE_KEY = "### Output:"
    END_KEY = "### End"

    blurb = f"\n{INTRO_BLURB}"

    # --- USING CORRECTED COLUMN NAMES ---
    # Define the input part using Tool, Category, and Description
    input_instruction = f"{INSTRUCTION_KEY}\n" \
                        f"Tool: {sample.get('Tool', 'N/A')}\n" \
                        f"Category: {sample.get('Category', 'N/A')}\n" \
                        f"Description: {sample.get('Description', 'N/A')}"

    # Define the desired output part using Command and Example_Usage
    output_response = f"{RESPONSE_KEY}\n" \
                      f"Command: {sample.get('Command', 'N/A')}\n" \
                      f"Example Usage: {sample.get('Example_Usage', 'N/A')}" # CORRECTED THIS LINE
    # ------------------------------------

    end = f"{END_KEY}"

    # Combine the parts for the training text
    parts = [part for part in [blurb, input_instruction, output_response, end] if part]

    # Add checks for empty or missing data if needed, although get handles basic missing keys
    if not all([sample.get('Tool'), sample.get('Category'), sample.get('Description'), sample.get('Command'), sample.get('Example_Usage')]):
         # Optionally skip samples with missing critical data
         # print(f"Skipping sample due to missing data: {sample}")
         return {"text": None} # Or return empty string, depending on desired handling

    formatted_prompt = "\n\n".join(parts)
    sample["text"] = formatted_prompt # Add the formatted text as a new column

    return sample

# %%
# This function prepares the dataset for training - it uses the column names
# found during the initial load dynamically. No changes needed here.

from functools import partial
from transformers import AutoTokenizer # Ensure AutoTokenizer is imported

def get_max_length(model):
    conf = model.config
    max_length = None
    for length_setting in ["n_positions", "max_position_embeddings", "seq_length"]:
        max_length = getattr(model.config, length_setting, None)
        if max_length:
            print(f"Found max lenth: {max_length}")
            break
    if not max_length:
        max_length = 1024
        print(f"Using default max length: {max_length}")
    return max_length

def preprocess_batch(batch, tokenizer, max_length):
    """
    Tokenizing a batch
    """
    return tokenizer(
        batch["text"],
        max_length=max_length,
        truncation=True,
    )

def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: DatasetDict):
    """Format & tokenize it so it is ready for training"""
    print("Preprocessing dataset...")

    # Apply create_prompt_formats first to add the 'text' column.
    # Filter out samples where create_prompt_formats returned {"text": None} or {"text": ""}
    dataset_with_text = dataset.map(create_prompt_formats).filter(lambda sample: sample.get("text") and len(sample["text"]) > 0)


    # Define columns to remove - original CSV columns + 'text'
    original_csv_columns = dataset['train'].column_names
    columns_to_remove_after_tokenization = original_csv_columns + ['text']

    print(f"Columns in dataset before tokenization map: {dataset_with_text['train'].column_names}")
    print(f"Columns to attempt removing after tokenization map: {columns_to_remove_after_tokenization}")


    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)

    # Apply tokenization and remove the specified columns
    processed_dataset = dataset_with_text.map(
        _preprocessing_function,
        batched=True,
        remove_columns=[col for col in columns_to_remove_after_tokenization if col in dataset_with_text['train'].column_names],
    )

    # Filter out samples that have input_ids exceeding max_length
    # Added check for 'input_ids' column existence
    if 'input_ids' in processed_dataset['train'].column_names:
        processed_dataset = processed_dataset.filter(lambda sample: len(sample["input_ids"]) < max_length)
    else:
        print("Warning: 'input_ids' column not found after tokenization. Cannot filter by max_length.")


    # Shuffle dataset
    processed_dataset = processed_dataset.shuffle(seed=seed)

    return processed_dataset

# %%
# Run this cell to preprocess and split the dataset.

# Ensure seed is defined (e.g., from an earlier cell like seed = 42)
# Make sure original_model and tokenizer are loaded from previous cells.

max_length = get_max_length(original_model) # Or the model you intend to train

# Preprocess the dataset loaded from CSV
processed_dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)

# Split the dataset into train and evaluation sets
train_eval_split = processed_dataset['train'].train_test_split(test_size=0.1, seed=seed) # Adjust test_size as needed
train_dataset = train_eval_split['train']
eval_dataset = train_eval_split['test']

print("\nProcessed train dataset:", train_dataset)
print("Processed eval dataset:", eval_dataset)

# %%
# Now you can proceed with your Trainer setup and training using
# train_dataset and eval_dataset.

# Example Trainer setup (assuming peft_model and peft_training_args are defined)
# from transformers import Trainer, DataCollatorForLanguageModeling # Ensure imports
# peft_trainer = Trainer(
#     model=peft_model,
#     train_dataset=train_dataset,
#     eval_dataset=eval_dataset,
#     args=peft_training_args, # Use your TrainingArguments object
#     data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
# )

# And then call peft_trainer.train()
# peft_trainer.train()

# %%
# Verify the structure and content of the processed datasets

# Check the structure of the train_dataset
print("\n--- Verifying Processed train_dataset ---")
print(train_dataset)
print("\nColumn names in train_dataset:")
print(train_dataset.column_names) # Should contain 'input_ids', 'attention_mask', and possibly 'labels'

# Check the structure of the eval_dataset
print("\n--- Verifying Processed eval_dataset ---")
print(eval_dataset)
print("\nColumn names in eval_dataset:")
print(eval_dataset.column_names) # Should match train_dataset

# %%
# Inspect a few samples from the processed train_dataset

print("\n--- Inspecting Processed Train Samples ---")
num_samples_to_inspect = 5 # Adjust as needed

for i in range(min(num_samples_to_inspect, len(train_dataset))):
    print(f"\n--- Sample {i+1}/{num_samples_to_inspect} ---")
    sample = train_dataset[i]

    # Print the tokenized output (input_ids and attention_mask)
    print("Tokenized Sample (input_ids):")
    print(sample.get('input_ids'))
    print("\nTokenized Sample (attention_mask):")
    print(sample.get('attention_mask'))

    # Decode the tokenized input_ids back to text to verify the formatting
    # We are training a Causal LM, so input_ids will be used as labels too by the data collator
    decoded_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=False) # Don't skip special tokens yet

    print("\nDecoded Text (with special tokens):")
    print(decoded_text)

    # Decode skipping special tokens to see the pure text content
    decoded_text_no_special = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)
    print("\nDecoded Text (without special tokens):")
    print(decoded_text_no_special)

    # --- Important Check ---
    # Verify that the decoded text matches the format you intended in create_prompt_formats
    # Look for the INTRO_BLURB, INSTRUCTION_KEY, RESPONSE_KEY, and END_KEY.
    # The model will be trained to predict the text following RESPONSE_KEY.

# %%
# Inspect a few samples from the processed eval_dataset (optional, usually similar to train)

print("\n--- Inspecting Processed Eval Samples ---")
num_samples_to_inspect = 3 # Adjust as needed

for i in range(min(num_samples_to_inspect, len(eval_dataset))):
    print(f"\n--- Eval Sample {i+1}/{num_samples_to_inspect} ---")
    sample = eval_dataset[i]

    # Print the tokenized output (input_ids)
    # print("Tokenized Sample (input_ids):")
    # print(sample.get('input_ids'))

    # Decode the tokenized input_ids back to text
    decoded_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=False)
    print("\nDecoded Text (with special tokens):")
    print(decoded_text)

    # --- Important Check ---
    # Again, verify the format of the decoded text. It should be the same
    # as the training data format.

# %%
# Verify the number of samples after filtering by max_length

print(f"\nNumber of samples in original dataset: {dataset['train'].num_rows}")
print(f"Number of samples in processed train_dataset: {len(train_dataset)}")
print(f"Number of samples in processed eval_dataset: {len(eval_dataset)}")
print(f"Total processed samples (train + eval): {len(train_dataset) + len(eval_dataset)}")

# The total processed samples should be less than or equal to the original number
# of samples, depending on how many were filtered out due to length.

# Ensure necessary imports for training are present
import transformers
from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling
# Ensure peft_model is defined (i.e., you've run the cells to load the base model,
# prepare it for kbit training, define the LoRA config, and get the peft_model)
# Ensure tokenizer is defined.
# Ensure output_dir and seed are defined (e.g., output_dir = f'./peft-training-{str(int(time.time()))}', seed = 42)
import time # Import time to ensure output_dir creation works


# Define your training arguments
# Review these parameters and adjust them based on your computational resources and desired training duration.
# These were likely defined in your original notebook.
output_dir = f'./peft-pen-testing-training-{str(int(time.time()))}' # Using a more descriptive output directory
peft_training_args = TrainingArguments(
    output_dir=output_dir,
    warmup_steps=1, # Number of warmup steps
    per_device_train_batch_size=1, # Batch size per GPU/core
    gradient_accumulation_steps=4, # Number of updates steps to accumulate before performing a backward/update pass
    max_steps=1000, # Total number of training steps to perform (instead of epochs) - adjust based on dataset size and desired training time
    learning_rate=2e-4, # The initial learning rate for AdamW optimizer
    optim="paged_adamw_8bit", # Optimizer
    logging_steps=25, # Log training and evaluation metrics every X steps
    logging_dir="./logs", # Directory for storing logs
    save_strategy="steps", # When to save checkpoints ('steps', 'epoch', 'no')
    save_steps=25, # Save checkpoint every X steps
    # Changed evaluation_strategy to eval_strategy as per traceback
    eval_strategy="steps", # When to evaluate ('steps', 'epoch', 'no')
    eval_steps=25, # Evaluate every X steps
    do_eval=True, # Whether to run evaluation during training
    gradient_checkpointing=True, # Enable gradient checkpointing to save memory
    report_to="none", # Reporting backend (e.g., "none", "wandb") - kept disabled as per your earlier code
    overwrite_output_dir=True, # Overwrite the output directory if it exists
    group_by_length=True, # Group samples with similar lengths to minimize padding
    # More parameters you might consider:
    # num_train_epochs=1, # Number of training epochs (use max_steps or epochs, not both)
    # lr_scheduler_type="cosine", # Learning rate scheduler type
    # weight_decay=0.001, # Weight decay for regularization
    # fp16=True, # Enable mixed precision training (requires compatible GPU)
    # logging_first_step=True,
    # save_total_limit=3, # Maximum number of checkpoints to keep
    # load_best_model_at_end=True, # Load the best model found during evaluation at the end of training
    # metric_for_best_model="eval_loss", # Metric to use for saving the best model
    # greater_is_better=False # For the metric_for_best_model
)

# Configure the model for training
# Disable caching if using gradient checkpointing
peft_model.config.use_cache = False

# Create the data collator
# Data collator for language modeling. Automatically pads samples to the longest length in the batch.
# mlm=False for causal language modeling (like with GPT-style models).
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)


# Initialize the Trainer
peft_trainer = Trainer(
    model=peft_model,         # Your PEFT-enabled model
    train_dataset=train_dataset, # Your processed training dataset from the CSV
    eval_dataset=eval_dataset,   # Your processed evaluation dataset from the CSV
    args=peft_training_args,  # Your training arguments
    data_collator=data_collator, # The data collator
)

print("Trainer initialized.")

# %%
# Start training!
print("Starting training...")
peft_trainer.train()
print("Training finished.")

# %%
# Optional: Save the final model after training
# The Trainer automatically saves checkpoints based on save_strategy and save_steps.
# However, saving the final model explicitly is good practice.
final_model_path = f"{output_dir}/final_checkpoint"
peft_trainer.save_model(final_model_path)
print(f"Final model saved to {final_model_path}")

# %%
# You can now proceed with loading the model for inference or pushing it to the hub
# using the final_model_path or one of the saved checkpoints.

peft_model_path = f"{output_dir}/final_checkpoint"

import time # Import the time module
output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'

# Mount Google Drive
    from google.colab import drive
    drive.mount('/content/drive')
    print("Google Drive mounted successfully!")